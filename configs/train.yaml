model: "gpt2" # option: ["gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]
num_steps: 8 # option: [2-8, None], if None, than run the baseline
wandb_mode: "online" # option: ["offline", "disabled"]
seed: 2020
num_epochs: 1
dataset_path: "dataset"
device: "cpu" # or "cpu"
batch_size: 2
learning_rate: 5e-4
validation_epochs: 1
checkpoint_dir: "checkpoints/"
repo_dir: "/Users/qyq/Documents/GitHub/cs4nlp"
batch_update: 10 # accumulate certain batches for gradient update
# "/cluster/scratch/yaqqin/MixtureOfExpertMathReasoning"
# transformers_cache_dir: None
